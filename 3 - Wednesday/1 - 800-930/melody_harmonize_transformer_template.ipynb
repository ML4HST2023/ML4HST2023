{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Import necessary libraries and modules----\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pretty_midi\n",
    "import time\n",
    "\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa5a4a",
   "metadata": {},
   "source": [
    "The following cell parses the both data set files and records some statistics on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Dataset exploration and vocabulary determination----\n",
    "\n",
    "# Use Python set object as a list which only stores unique elements (no duplicates)\n",
    "mel_vocab = set() # Set to contain all unique melody notes\n",
    "harm_vocab = set() # Set to contain all unique harmony sequences (3 note groups)\n",
    "# Create list of file paths for train and validation numpy data files\n",
    "paths = [\"train_data.npy\",\n",
    "         \"val_data.npy\"]\n",
    "\n",
    "# Iterate through files\n",
    "for f in paths:\n",
    "    # Load each file\n",
    "    data = np.load(f)\n",
    "    arr_shape = np.shape(data)\n",
    "    # Iterate through each chorale in data file\n",
    "    for i in range(arr_shape[0]):\n",
    "        # Iterate through each chord in chorale\n",
    "        for j in range(arr_shape[1]):\n",
    "            # Iterate through each potential transposition distance (including none as 0)\n",
    "            for o in [0]:\n",
    "            #for o in [-2, -1, 0, 1, 2]:\n",
    "                # All chords after the end of the piece are encoded as all 0\n",
    "                if data[i][j][0] == 0:\n",
    "                    # If weve reached the end of the chorale, don't add to vocab\n",
    "                    break \n",
    "                # Add offset to melody note and add it to melody vocabulary\n",
    "                mel_vocab.add(data[i][j][0] + o)\n",
    "                # Add same offset to all harmony notes and add that sequence to the harmony vocab\n",
    "                aug_harm = (data[i][j][1] + o, data[i][j][2] + o, data[i][j][3] + o)\n",
    "                harm_vocab.add(aug_harm)\n",
    "\n",
    "print(\"Unique notes after complete data set augmentation: \", len(mel_vocab))\n",
    "print(\"Unique chords after complete data set augmentation: \", len(harm_vocab))            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb532fa0",
   "metadata": {},
   "source": [
    "We must create a **Vocabulary** for the harmony and melody. This is a mapping between unique notes/chords and the values that we input to the transformer. Transformers use an *embedding* layer at the input to transform input data into a dense vector in the \"embedding\" space, and relations between different input data items are encoded in the embedding space. We need to have integer **tokens** for each unique item in both the melody and harmony vocabularies.\\\n",
    "\\\n",
    "We also need to include three more items in each vocabulary:\n",
    "- A **Start-of-Sequence** token (SOS), indicating the start of the piece.\n",
    "- An **End-of-Sequence** token (EOS), indicating the end of the piece.\n",
    "- A **padding** token, used to make all chorales in the dataset a uniform size to store them together in arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a87b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Use python dictionaries to map between notes/chords and tokens----\n",
    "\n",
    "# We choose 1 as our SOS token, 100 as our EOS token, and 0 as our padding token\n",
    "# Create dict for harmony vocabulary: keys are the tuples of unique MIDI number chords, values are integers\n",
    "harm_dict_seq_2_idx = {(0,0,0): len(harm_vocab), (1,1,1): len(harm_vocab)+1, (100, 100, 100): len(harm_vocab)+2} \n",
    "# Create dict for melody vocabulary: keys are unique MIDI numbers in vocab\n",
    "mel_dict_note_2_idx = {0: len(mel_vocab), 1: len(mel_vocab)+1, 100: len(mel_vocab)+2}\n",
    "\n",
    "#loop through harmony set to add all unique chords to dictionary\n",
    "for i, seq in enumerate(harm_vocab):\n",
    "    harm_dict_seq_2_idx[seq] = i\n",
    "\n",
    "#loop through melody set to add all unique notes to dictionary   \n",
    "for i, note in enumerate(mel_vocab):\n",
    "    mel_dict_note_2_idx[note] = i\n",
    "\n",
    "    \n",
    "# Use dict comprehension to create reverse dictionaries (key <---> value) to reverse the lookup later\n",
    "harm_dict_idx_2_seq = {v: k for k,v in harm_dict_seq_2_idx.items()}\n",
    "mel_dict_idx_2_note = {v: k for k,v in mel_dict_note_2_idx.items()}\n",
    "\n",
    "#----Define functions to perform mapping on arrays----\n",
    "def get_harm_index_from_vocab(data):\n",
    "    t = torch.tensor([harm_dict_seq_2_idx[tuple(seq)] for seq in data])\n",
    "    return t\n",
    "\n",
    "def get_harm_seq_from_idx(data):\n",
    "    data = data[0]\n",
    "    t = []\n",
    "    for i in range(642):\n",
    "        t.append(list(harm_dict_idx_2_seq[data[i].item()]))\n",
    "    return np.asarray(t)\n",
    "    \n",
    "def get_mel_index_from_vocab(data):\n",
    "    t = torch.tensor([mel_dict_note_2_idx[n.item()] for n in data])\n",
    "    return t\n",
    "\n",
    "def get_mel_note_from_idx(data):\n",
    "    data = data[0]\n",
    "    t = []\n",
    "    for n in range(642):\n",
    "        t.append(mel_dict_idx_2_note[data[n].item()])\n",
    "    return np.asarray(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7811fc69",
   "metadata": {},
   "source": [
    "To load the training data from files, we'll need a data set class to pass to our PyTorch Dataloader. This class will implement the particulars of extracting the melody and harmony from the data array, insertion of special tokens and padding, data transformations, and setting the size of an individual training sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d941285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class which inherits from torch.utils.data.Dataset\n",
    "class MusicDataSetMIDI(Dataset):\n",
    "    def __init__(self, data_array_file, sos, eos, transform=None):\n",
    "        # read data into array from file\n",
    "        self.data_arr = np.load(data_array_file)\n",
    "        # start of sequence token\n",
    "        self.sos = sos \n",
    "        # end of sequence token\n",
    "        self.eos = eos \n",
    "        # any data augmentation functions\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        #first dimension of data tensor represents number of data   \n",
    "        return np.shape(self.data_arr)[0] #first dimension of data tensor represents number of data   \n",
    "    \n",
    "\n",
    "    def insert_tokens(self, chorale):\n",
    "        # Create a row of SOS token\n",
    "        row_sos = np.full((1, chorale.shape[1]), self.sos, dtype=chorale.dtype)\n",
    "        # Create a row vector of all EOS token\n",
    "        row_eos = np.full((1, chorale.shape[1]), self.eos, dtype=chorale.dtype)\n",
    "        # Insert the row of SOS token at the beginning\n",
    "        chorale = np.vstack((row_sos, chorale))\n",
    "        # Find the first row of all zeros\n",
    "        #print(\"Chorale: \", chorale)\n",
    "        #print(\"Chorale shape: \", np.shape(chorale))\n",
    "        first_zero_row = np.where(np.all(chorale == 0, axis=1))[0][0]\n",
    "        # Insert the row vector of all EOS token before the first zero row\n",
    "        arr = np.insert(chorale, first_zero_row, row_eos, axis=0)\n",
    "        return arr\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chorale = self.data_arr[idx,:,:] #select chorale 'idx' with shape (640, 4)\n",
    "        # If we have passed transformation functions, apply them to the chorale\n",
    "        if self.transform:\n",
    "            chorale = self.transform(chorale)\n",
    "        # Call class method to insert SOS and EOS tokens\n",
    "        chorale = self.insert_tokens(chorale)\n",
    "        # Find index in array where first row of zeros occurs\n",
    "        first_zero_idx = np.where(np.all(chorale==0, axis=1))[0][0]\n",
    "        # Compute number of sections with notes based on our desired training sequence length\n",
    "        sections = int(first_zero_idx / train_seq_length) + 1\n",
    "        # Randomly select a section\n",
    "        sec_sel = np.random.randint(sections)\n",
    "        # Slice chorale into its section of length 'train_seq_length' and split array into melody and harmony\n",
    "        mel = chorale[sec_sel*train_seq_length:train_seq_length*(sec_sel+1), 0]\n",
    "        harm = chorale[sec_sel*train_seq_length:train_seq_length*(sec_sel+1), 1:]\n",
    "        # Pass melody and harmony sequences through mapping to change values into vocabulary indices\n",
    "        m = get_mel_index_from_vocab(mel)\n",
    "        h = get_harm_index_from_vocab(harm)\n",
    "        \n",
    "        return m, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbe812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Set up data loaders----\n",
    "# Absolute file paths for data files\n",
    "\n",
    "\n",
    "\n",
    "# Define special tokens to pass to data set object\n",
    "\n",
    "\n",
    "\n",
    "# Create data set objects with data file paths\n",
    "\n",
    "\n",
    "\n",
    "# Define training/validation sequence length\n",
    "\n",
    "\n",
    "# Define batch size to pass to data loaders\n",
    "\n",
    "\n",
    "# Instantiate data loader objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949876c5",
   "metadata": {},
   "source": [
    "Now we'll define the Transformer model in the following two cells as follows:\n",
    "- PositionalEncoding is a class which defines the operation of encoding the input and target sequences in a manner\n",
    "  which encodes its position in the sequence. This serves to give the transformer additional context as to the temporal\n",
    "  relationship between elements in the sequences.\n",
    "- Transformer is a class which defines the operations of the forward pass of the Transformer model. This includes token       embedding, positional encoding, and the Transformer model itself. This class also contains a function to generate target   masks, which we give to the transformer so that it may not attend to time steps in the future during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b989983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_length):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        dim_model : dimension of the embedded representations used as inputs to the multi-head attention blocks\n",
    "        dropout_p : probability of dropout in all dropout layers\n",
    "        max_len   : how far the position can have an effect on a token (like a window of influence)\n",
    "        \"\"\"\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        #Encoding from formula\n",
    "        pos_encoding = torch.zeros(max_length, 1, dim_model)\n",
    "        positions_list = torch.arange(max_length).unsqueeze(1)\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)\n",
    "\n",
    "        #pos_encoding(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0, 0::2] = torch.sin(positions_list * division_term)\n",
    "        #pos_encoding(pos, 2i+1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0, 1::2] = torch.cos(positions_list * division_term)\n",
    "\n",
    "        #save as buffer (the same as a parameter but without gradients)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "            \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        #residual connection + positional encoding\n",
    "        token_plus_pos_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]\n",
    "        #send through dropout layer and return\n",
    "        return self.dropout(token_plus_pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    #Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        melody_vocab_size,\n",
    "        harmony_vocab_size,\n",
    "        dropout_p,\n",
    "        pos_enc_max_len\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        \n",
    "        # LAYERS\n",
    "        \n",
    "        #Give transformer model an object of our positional encoding class\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_length=pos_enc_max_len\n",
    "        )\n",
    "\n",
    "        #Create embedding layer to turn sequence vectors into dense, continuous vector space\n",
    "        self.source_embedding = nn.Embedding(melody_vocab_size, dim_model)\n",
    "        self.target_embedding = nn.Embedding(harmony_vocab_size, dim_model)\n",
    "        \n",
    "        #Create transformer layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model, \n",
    "            nhead=num_heads, \n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, harmony_vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.source_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.target_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out.bias.data.zero_()\n",
    "        self.out.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):\n",
    "        #src size must be (batch_size, src sequence length)\n",
    "        #tgt size must be (batch_size, tgt sequence length)\n",
    "        \n",
    "        #Embedding + positional encoding: output size = (batch size, sequence length, dim_model)\n",
    "        src = self.source_embedding(src) * math.sqrt(self.dim_model)\n",
    "        tgt = self.target_embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "        #Permute embedded/encoded sequences to obtain shape (seqence length, batch size, dim_model)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        \n",
    "        #Pass embedded/encoded sequences to transformer\n",
    "        transformer_out = self.transformer(\n",
    "            src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask\n",
    "        )\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        #Generates a square matrix where each row allows one more 'word' of sequence to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1)\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask==0, float('-inf'))\n",
    "        mask = mask.masked_fill(mask==1, float(0.0))\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f5e657",
   "metadata": {},
   "source": [
    "Now we will instantiate the model and create a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for training and testing the model\n",
    "\n",
    "\n",
    "#--------Arguments for transformer model-----------\n",
    "\n",
    "# Size of the melody and harmony vocabularies to pass to the embedding layers in the model\n",
    "\n",
    "\n",
    "# Size of latent embedded vectors passed to the model. Hyperparameter open to tuning\n",
    "\n",
    "# Number of attention heads in multi headed attention modules. Hyperparameter open to tuning\n",
    "\n",
    "# Number of stacked encoder/decoder layers. Hyperparameter open to tuning.\n",
    "\n",
    "\n",
    "# Probability of dropout layers\n",
    "\n",
    "# Maximum sequence length. Make sure this is higher than the selected sequence length for the data loader\n",
    "\n",
    "# Instantiate Transformer model\n",
    "model = Transformer()\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "\n",
    "# Since this is fundamentally a multiclass classification along the harmony vocabulary, use cross entropy loss function,\n",
    "# which is ideally suited for multiclass classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Training loop ----\n",
    "def training_loop(model, opt, loss_fn, dataloader):\n",
    "    \n",
    "    # Set model to train mode to enable dropout layers\n",
    "    model.train()\n",
    "    # Initialize value to track training statistics\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate through the entire training dataloader\n",
    "    for input_melody, target_harmony in dataloader:\n",
    "\n",
    "        # Send data to devices\n",
    "        input_melody = torch.tensor(input_melody).to(device)\n",
    "        target_harmony = torch.tensor(target_harmony).to(device)\n",
    "        \n",
    "        # Shift target sequences so that when model sees SOS in input sequence, it predicts the token at pos. 1 \n",
    "        target_input = target_harmony[:, :-1]\n",
    "        target_expected = target_harmony[:, 1:]\n",
    "\n",
    "        # Generate target masks\n",
    "        sequence_length = target_input.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        \n",
    "        # Forward pass of melody, target harmony, and mask through model\n",
    "        pred = model(input_melody, target_input, tgt_mask)\n",
    "        \n",
    "        # Rearrange output to have batch size first to pass to loss function\n",
    "        pred = pred.permute(1, 2, 0)\n",
    "\n",
    "        # Pass through loss function and perform backpropogation\n",
    "        loss = loss_fn(pred, target_expected)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Add to running epoch loss\n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "        \n",
    "#----Validation loop----\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \n",
    "    # Set model to eval mode to disable dropout layers\n",
    "    model.eval()\n",
    "    # Initialize value to track training statistics\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use the no_grad context manager to disable collection of gradients during evaluation,\n",
    "    # reducing memory useage and speeding up validation loop\n",
    "    with torch.no_grad():\n",
    "        # Iterate through the entire validation dataloader\n",
    "        for input_melody, target_harmony in dataloader:\n",
    "            # Send data to devices\n",
    "            input_melody = input_melody.to(device)\n",
    "            target_harmony = target_harmony.to(device)\n",
    "        \n",
    "            # Shift target sequences so that when model sees SOS in input sequence, it predicts the token at pos. 1 \n",
    "            target_input = target_harmony[:, :-1]\n",
    "            target_expected = target_harmony[:, 1:]\n",
    "\n",
    "            # Generate target masks\n",
    "            sequence_length = target_input.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        \n",
    "            # Forward pass of melody, target harmony, and mask through model\n",
    "            pred = model(input_melody, target_input, tgt_mask)\n",
    "        \n",
    "            # Rearrange output to have batch size first to pass to loss function\n",
    "            pred = pred.permute(1, 2, 0)\n",
    "\n",
    "            loss = loss_fn(pred, target_expected)\n",
    "            \n",
    "            total_loss += loss.detach().item()\n",
    "    return total_loss / len(dataloader)\n",
    "    \n",
    "# Number of training epochs\n",
    "\n",
    "# Collect list of epoch losses for plotting\n",
    "train_loss_list, val_loss_list = [], []\n",
    "start = time.time()\n",
    "for n in range(num_epochs):\n",
    "    print('-' * 30)\n",
    "    print(\"Epoch | \", n+1)\n",
    "    \n",
    "    # Train for one epoch\n",
    "\n",
    "    # Add training loss to list\n",
    "\n",
    "    \n",
    "    # Run inference on validation data\n",
    "\n",
    "    # Add validation loss to list\n",
    "\n",
    "    \n",
    "    #Print statistics\n",
    "    print(f'Training loss: {train_loss:.4f}  |  Validation loss: {val_loss:.4f}')\n",
    "    print('-' * 30)\n",
    "    \n",
    "stop = time.time()\n",
    "train_time = stop-start\n",
    "print(f'Training time: {train_time:.4f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Plotting training loss----\n",
    "\n",
    "# List for x-axis\n",
    "epochs = [i+1 for i in range(num_epochs)]\n",
    "\n",
    "plt.plot(epochs, train_loss_list, color='blue', label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_list, color='red', label=\"Validation loss\")\n",
    "\n",
    "#Adding axis labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per epoch')\n",
    "#Add legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49faabdb",
   "metadata": {},
   "source": [
    "Now with the model, we need to define a function which takes a melody sequence and performs an inference (forward pass) on the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, melody_seq):\n",
    "    \n",
    "    # set model to evaluation mode to turn off Dropout/BatchNorm/other functionalities which are used for training\n",
    "    # but should not be used on inference of a trained model\n",
    "    model.eval()\n",
    "    \n",
    "    # Find length of melody sequence: melody tensor is of shape (1, L), so the length L of \n",
    "    # the sequence is the second element in the return value from tensor.size()\n",
    "    seq_length = melody_seq.size()[1]\n",
    "    print(\"Length of melody sequence: \", seq_length)\n",
    "    \n",
    "    # initialize empty list to store the predicted harmony at each time step\n",
    "    harmony = []\n",
    "    \n",
    "    # initialize a \"previous output\" tensor to pass to the Transformer decoder. Eventually this will be\n",
    "    # appended with the predictions to give the decoder more of the sequence for context, but here we will \n",
    "    # use the SOS token\n",
    "    harm_input = torch.tensor([[1]], dtype=torch.long, device=device)\n",
    "    \n",
    "    # loop as many times as there are elements in the input melody\n",
    "    for _ in range(seq_length):\n",
    "        \n",
    "        # Get target mask from Transformer model class method to stop decoder from attending to future inputs while\n",
    "        # predicting the current input\n",
    "        tgt_mask = model.get_tgt_mask(harm_input.size(1)).to(device)\n",
    "        \n",
    "        # The model will output a tensor with its last dimension of shape V, where V is the number of elements in the \n",
    "        # harmony vocab. The values for these V elements can be seen as probabilities that each element is the \"proper\"\n",
    "        # selection.\n",
    "        # Forward pass of model, requires input sequence, harmony input to decoder, and target mask as arguments\n",
    "        prediction = model(melody_seq, harm_input, tgt_mask)\n",
    "        \n",
    "        # Use PyTorch 'topk' function to select the 5 largest values in the predictions\n",
    "        top_5 = torch.topk(prediction, 5)\n",
    "        \n",
    "        # Topk returns a tuple of tensors, the first tensor has the highest 5 values, and the second tensor\n",
    "        # has the indices where those top 5 values were found.\n",
    "        top_5_values = top_5[0][0]\n",
    "        top_5_indices = top_5[1][0][0]\n",
    "        \n",
    "        # Now, sample a single value from the probability distribution of the top 5 values using a multinomial\n",
    "        # distribution. This will select one value in the top 5 with likelihood proportional to its value.\n",
    "        next_item = torch.multinomial(top_5_values, 1)\n",
    "        \n",
    "        # torch.multinomial returns the indices of the selected values in the tensor, so use that index to get the \n",
    "        # \"vocab index\".\n",
    "        next_index = top_5_indices[next_item[-1]]\n",
    "        \n",
    "        # Append the \"vocab index\" to our harmony list\n",
    "        harmony.append(next_index.item())\n",
    "        \n",
    "        # Recast vocab index as a torch tensor of the correct dimension and add it to the 'previous outputs'\n",
    "        # tensor to feed to the decoder in the next loop\n",
    "\n",
    "        next_index = torch.tensor([[next_index]], device=device)\n",
    "        harm_input = torch.cat((harm_input, next_index), dim=1)\n",
    "        \n",
    "    #After loop is done, return the harmony list\n",
    "    return harmony\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5101012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from melody_harm_utilities import melody_sequence_to_index, reassemble_sequences\n",
    "import os\n",
    "\n",
    "# Test melody - Twinkle Twinkle Little Star\n",
    "melody = [67, 67, 67, 67, 74, 74, 74, 74, 76, 76, 72, 72, 74, 74, 71, 71, \n",
    "          72, 72, 69, 69, 71, 71, 67, 67, 69, 69, 66, 66, 67, 67, 66, 66]\n",
    "\n",
    "\n",
    "nb_dir = os.getcwd()\n",
    "# Change path and uncomment to load melody array from song requests\n",
    "#melody = np.load(os.path.join(nb_dir, 'samples\\\\superstition.npy))\n",
    "\n",
    "#----Use helper functions to run inference on melody sequence----\n",
    "# convert from MIDI to vocab index\n",
    "melody = melody_sequence_to_index(melody, mel_dict_note_2_idx)\n",
    "# send melody tensor to compute device\n",
    "melody = melody.to(device) \n",
    "# Call function to run inference on melody\n",
    "harmony = run_inference(model, melody) \n",
    "# Reassemble melody and predicted harmony into 4 part harmony sequence\n",
    "chorale = reassemble_sequences(melody, harmony, mel_dict_idx_2_note, harm_dict_idx_2_seq)\n",
    "# Print melody and harmony sequences (in indices rather than MIDI numbers) and reassembled piece\n",
    "print(\"Melody: \", melody)\n",
    "print(\"Harmony: \", harmony)\n",
    "print(\"Reassembled piece: \", chorale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47759873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from melody_harm_utilities import piano_roll_from_sequence, piano_roll_to_midi\n",
    "\n",
    "# Use helper functions to convert symbolic music into MIDI format\n",
    "chorale_piano_roll = piano_roll_from_sequence(chorale, 10)\n",
    "chorale_midi = piano_roll_to_midi(chorale_piano_roll, 2)\n",
    "\n",
    "# Synthesize generated music into audio\n",
    "import IPython.display\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
