{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "558b9f0b",
   "metadata": {},
   "source": [
    "Vehicle Detection with Faster RCNN and Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43fb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Housekeeping----\n",
    "CLASSES = 2 # car + background\n",
    "\n",
    "# Set compute device (GPU or CPU)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Training data and annotations paths\n",
    "train_data_path = 'vehicle_data/train_images/'\n",
    "train_annotation_path = 'train_annotations.json'\n",
    "# Validation data and annotation paths\n",
    "val_data_path = 'vehicle_data/val_images/'\n",
    "val_annotation_path = 'val_annotations.json'\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdf64c",
   "metadata": {},
   "source": [
    "Now we must construct our data set object. This inherits from the PyTorch *Dataset* class, and we must rewrite some class methods for our specific problem. We use *pycocotools* to read our JSON annotation file. See the [documentation](https://pytorch.org/vision/0.12/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html#torchvision.models.detection.fasterrcnn_resnet50_fpn) for our model to understand why we format the data in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd00b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Data loader----\n",
    "\n",
    "import utils #downloaded from torch/vision/references/detection\n",
    "class VehicleDetectionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images_root, annot_file, transform=None, target_transform=None):\n",
    "        from pycocotools.coco import COCO # Module for parsing JSON annotation files in COCO format\n",
    "        self.img_root = images_root # Path to image directory\n",
    "        self.coco = COCO(annot_file) # COCO object representing annotations\n",
    "        self.ids = list(self.coco.imgs.keys()) # List of unique image identifiers in annotation file\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and corresponding annotations from index\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx] \n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        \n",
    "        img = Image.open(os.path.join(self.img_root, path)).convert('RGB')\n",
    "        \n",
    "        # Perform transformations on images if desired\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        # Our images contain other annotations for other objects that we don't care about. So we'll loop through\n",
    "        # each label in the image and retain only those which are of vehicles.\n",
    "        \n",
    "        # Initialize lists\n",
    "        bbox = []\n",
    "        area = []\n",
    "        image_id = []\n",
    "        class_labels = []\n",
    "        empty_check = 0\n",
    "        # Loop through each annotation in the image\n",
    "        for i in target:\n",
    "            if i['category_id'] == 1: #Indicating box is for a car\n",
    "                # Convert bounding box format from (xmin, ymin, width, height) to (xmin, ymin, xmax, ymax)\n",
    "                box_convert = [i['bbox'][0], \n",
    "                               i['bbox'][1], \n",
    "                               i['bbox'][0] + i['bbox'][2], \n",
    "                               i['bbox'][1] + i['bbox'][3]]\n",
    "                # Append targets of relevant annotations to lists\n",
    "                bbox.append(box_convert)\n",
    "                area.append(i['bbox'][2] * i['bbox'][3])\n",
    "                image_id.append(i['image_id'])\n",
    "                class_labels.append(i['category_id'])\n",
    "                empty_check += 1\n",
    "        \n",
    "        # Per documentation of our pretrained model, our training target must be a dictionary with the keys as specified\n",
    "        # and the values as pytorch tensors.\n",
    "        target_dict = {}\n",
    "        target_dict[\"boxes\"] = torch.as_tensor(bbox)\n",
    "        target_dict[\"labels\"] = torch.as_tensor(class_labels)\n",
    "        target_dict[\"image_id\"] = torch.as_tensor(image_id)\n",
    "        target_dict[\"area\"] = torch.as_tensor(area)\n",
    "        \n",
    "        target_list_of_dict = []\n",
    "        target_list_of_dict.append(target_dict)\n",
    "        if empty_check == 0:\n",
    "            print(path)\n",
    "            \n",
    "            \n",
    "        return img, target_dict\n",
    "    \n",
    "    \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor() ])\n",
    "\n",
    "#Instantiate data set object and create a data loader\n",
    "train_dataset = VehicleDetectionDataset(train_data_path, train_annotation_path, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "val_dataset = VehicleDetectionDataset(val_data_path, val_annotation_path, transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "num_train_images = len(train_dataset)\n",
    "num_val_images = len(val_dataset)\n",
    "\n",
    "print(\"Number of images in training set: \", num_train_images)\n",
    "print(\"Number of images in validation set: \", num_val_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3ee28",
   "metadata": {},
   "source": [
    "Now before we get into the model, we'll define some helper functions for viewing images and imposing bounding boxes on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23873959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Function to pre-process frame for inference, transform image data to be operable with PyTorch ----\n",
    "def prepare_img_for_inference(img, resize=False):\n",
    "    # Keep copy of image for displaying\n",
    "    original = img.copy()\n",
    "    \n",
    "    # Transform from BGR channel order of cv2 to RGB channel order for pytorch\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    image = image.transpose((2,0,1))\n",
    "    \n",
    "    # Add batch dimension\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    # Rescale between [0,1]\n",
    "    image = image / 255.0\n",
    "    # Recast to PyTorch tensor\n",
    "    image = torch.FloatTensor(image)\n",
    "    \n",
    "    # Send to device\n",
    "    image = image.to(device)\n",
    "    \n",
    "    return image, original\n",
    "\n",
    "\n",
    "#---- Function which takes model prediction and imposes predicted bounding boxes on the image for display ----\n",
    "def display_image_and_inferences(detections, orig_image, conf_threshold, display=False):\n",
    "    # Loop through all predicted bounding boxes\n",
    "    for i in range(0, len(detections[\"boxes\"])):\n",
    "        # extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[\"scores\"][i]\n",
    "        # filter out weak detections by ensuring the confidence is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > conf_threshold:\n",
    "            # extract the index of the class label from the detections,\n",
    "            # then compute the (x, y)-coordinates of the bounding box\n",
    "            # for the object\n",
    "            idx = int(detections[\"labels\"][i])\n",
    "            box = detections[\"boxes\"][i].detach().cpu().numpy()\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # display the prediction to our terminal\n",
    "            label = \"Car: {:.2f}%\".format(confidence * 100)\n",
    "            print(\"[INFO] {}\".format(label))\n",
    "            # draw the bounding box and label on the image\n",
    "            cv2.rectangle(orig_image, (startX, startY), (endX, endY),\n",
    "                (0,0,255), 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(orig_image, label, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "            \n",
    "    # Used for inline display of sample\n",
    "    if display:\n",
    "        plt.imshow(orig_image)\n",
    "        plt.show()\n",
    "    \n",
    "    # Return the image with bounding boxes drawn\n",
    "    return orig_image\n",
    "\n",
    "\n",
    "#---- Function to take test video, make model predictions frame by frame, and display annotations in video ----\n",
    "def annotate_video(model, video_path, conf):\n",
    "    # Create opencv VideoCapture object to read MP4 file\n",
    "\n",
    "    # Check if VideoCapture has opened video without problem\n",
    "    if not video.isOpened():\n",
    "        print(\"VideoCapture initialization failed!\")\n",
    "        return None\n",
    "    # Read initial frame from video\n",
    "\n",
    "    # Flag to indicate if video is playing or paused\n",
    " \n",
    "    # Do not collect gradients to speed up inferences\n",
    "\n",
    "        # The first return of VideoCapture.read() will become false when the video is out of frames\n",
    "        while ret_val:\n",
    "            # Only resume inferences if playing flag is True\n",
    "            if playing:\n",
    "                # Use function to pre-process the frame for our model\n",
    "\n",
    "                # Perform inference on video frame\n",
    "\n",
    "                # Use defined function to draw bounding boxes from inference onto frame\n",
    "\n",
    "                # Place frame onto display window\n",
    "                cv2.imshow(\"Video Model Predictions\", display_frame)\n",
    "            # Block for minimal amount and proceed\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # If 'q' key is pressed, stop the process and return\n",
    "            if key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                video.release()\n",
    "                return None\n",
    "            # If 'p' key is pressed, toggle between pause and play\n",
    "            if key == ord('p'):\n",
    "                playing = not playing\n",
    "            # Grab next frame from video\n",
    "            \n",
    "            \n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64836ef1",
   "metadata": {},
   "source": [
    "Now we'll load the model and perform a sample inference to see how it works. Note, the pre-trained model already classifies cars, we are applying this particular transfer learning as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585afef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "rcnn_model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    pretrained=True, progress=True, pretrained_backbone=True)\n",
    "\n",
    "# Send model to compute device and set evaluation mode to dictate the behavior of certain layers (dropout, batchnorm, etc.)\n",
    "rcnn_model.to(device)\n",
    "rcnn_model.eval()\n",
    "\n",
    "# Load and process sample image\n",
    "sample_img_path = \"test_image.png\"\n",
    "sample_img = cv2.imread(sample_img_path)\n",
    "sample_img_inference, original_sample = prepare_img_for_inference(sample_img)\n",
    "# Run inference and display predicted classifications and detections on original image\n",
    "sample_output = rcnn_model(sample_img_inference)[0]\n",
    "_ = display_image_and_inferences(sample_output, original_sample, 0.7, display=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze weights in network\n",
    "for param in rcnn_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#---Reset final fully connected layer to reflect desired number of classes---\n",
    "num_features = rcnn_model.roi_heads.box_predictor.cls_score.in_features\n",
    "rcnn_model.roi_heads.box_predictor = FastRCNNPredictor(num_features, CLASSES)\n",
    "#Place model on compute device\n",
    "rcnn_model = rcnn_model.to(device)\n",
    "\n",
    "#create optimizer - note that only final layer parameters are being optimized\n",
    "params = [p for p in rcnn_model.parameters() if p.requires_grad]\n",
    "opt = optim.SGD(params, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cfa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate #downloaded from torch/vision/references/detection\n",
    "\n",
    "# Set some number of epochs and train\n",
    "num_epochs = 10\n",
    "for ep in range(num_epochs):\n",
    "    train_one_epoch(rcnn_model, opt, train_dataloader, device, ep, print_freq=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_model.eval()\n",
    "\n",
    "# Run inference and display predicted classifications and detections on original image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75d086",
   "metadata": {},
   "source": [
    "Now we'll load our video and pass it through our model to predict vehicles in the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
