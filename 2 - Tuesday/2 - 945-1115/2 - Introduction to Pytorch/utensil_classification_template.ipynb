{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a688f0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import numerical python libraries to create vectors and perform matrix multiplication\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Import pyplot plotting library from matplotlib to plot similar to Matlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Image library from PIL (pillow) to load images for display purposes\n",
    "\n",
    "\n",
    "# Import necessary Pytorch libraries for building neural networks, optimization, and dataloading\n",
    "\n",
    "\n",
    "\n",
    "# Import a visualization library to display a summary of the models we build\n",
    "\n",
    "\n",
    "# Import some helpful metrics to help us understand the models performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import a locally developed library to perform printing functions\n",
    "import uwyo_common as common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f17eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    build_model(image_width, image_height)\n",
    "    A function to build and test pytorch models. This function is static and must change when a new\n",
    "    model is desired.\n",
    "    \n",
    "    inputs:\n",
    "     - image_width (int): The expected image weight to be processed\n",
    "     - image_height (int): The expected image height to be processed\n",
    "    outputs:\n",
    "     - model (pytorch model): The resultant pytorch model\n",
    "\"\"\"\n",
    "def build_model(image_width, image_height):\n",
    "    \n",
    "    model = nn.Sequential() # Start with a blank model with no layers\n",
    "    dummy = torch.ones((1, 1, image_width, image_height)) # 'Dummy' image to check outputs at each layer\n",
    "    \n",
    "    # Add Convolution layers for efficient image processing\n",
    "    # Add pooling layers to both reduce the image size and to summarize important features\n",
    "    # Add batch normalization layers to reduce covariate shift \n",
    "    # The proper layer order is convolution -> pooling -> batch normalization (repeat)\n",
    "    layer_name = 'conv1'\n",
    "     \n",
    "    layer_name = 'pool1'\n",
    "    \n",
    "    layer_name = 'batch norm 1'\n",
    "    \n",
    "    layer_name = 'conv2'\n",
    "\n",
    "    layer_name = 'pool2'\n",
    "    \n",
    "    layer_name = 'batch norm 2'\n",
    "\n",
    "    # Add a flatten layer to transition from convolution layers to linear layers\n",
    "    # Add linear layers to increase model complexity\n",
    "    # The proper layer order is flatten -> linear layers -> output\n",
    "    layer_name = 'flatten'\n",
    "\n",
    "    layer_name = 'linear 1'\n",
    "\n",
    "    # Add a linear layer for the output, this is always your last layer\n",
    "    layer_name = 'output'\n",
    "    \n",
    "    summary(model, (1,image_width, image_height))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1242e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    load_data(path, batch_siz, image_width, image_height, transform_train, transform_test, valid, display)\n",
    "    A function to automate loading image datasets to be used for training and testing\n",
    "    pytorch image models. If the image folder is not divided into training and testing \n",
    "    or training and validation and testing, then it is assumes the images are sorted by \n",
    "    class and will be divided into an 80 - 20 or 80 - 10 - 10 split depending on valid\n",
    "    boolean value.\n",
    "    \n",
    "    inputs:\n",
    "     - path (string): Path to the base folder where the images have been sorted.\n",
    "     - batch_size (int): The number of examples per batch\n",
    "     - image_width (int): The transformed image width\n",
    "     - image_height (int): The transformed image height\n",
    "     - transform_train (pytorch transform): A transformation to use when loading training images\n",
    "     - transform_test (pytorch transform): A tranformation to use when loading testing images\n",
    "     - valid (boolean): Is there a validation dataset?\n",
    "     - display (boolean): Should examples be shown?\n",
    "    outputs:\n",
    "     - train_dl (pytorch dataloader): The dataloader for the training dataset\n",
    "     - valid_dl (pytorch dataloader): The dataloader for the validation dataset\n",
    "     - tests_dl (pytorch dataloader): The dataloader for the testing dataset\n",
    "     - labels (list): The strings representing the text labels\n",
    "\"\"\"\n",
    "def load_data(path, batch_size=64, image_width=64, image_height=64, transform_train=None, transform_test=None, valid=True, display=False):\n",
    "    # Transform to be used on our training/validation dataset if none were specified\n",
    "    if transform_train is None:\n",
    "        transform_train = transforms.Compose([transforms.Resize([image_width, image_height]),\n",
    "                                              transforms.Grayscale(num_output_channels=1),\n",
    "                                              transforms.ToTensor(), ])\n",
    "    \n",
    "    # Transform to be used on our testing dataset if none were specified\n",
    "    if transform_test is None:\n",
    "        transform_test = transforms.Compose([transforms.Resize([image_width, image_height]),\n",
    "                                             transforms.Grayscale(num_output_channels=1),\n",
    "                                             transforms.ToTensor(), ])\n",
    "    \n",
    "    dictionary = {'label name':[],'files':[],'label int':[]}\n",
    "    \n",
    "    try:\n",
    "        # Create datasets from the specified presplit path\n",
    "        train_dataset = datasets.ImageFolder(path + '/training/', transform=transform_train)\n",
    "        valid_dataset = datasets.ImageFolder(path + '/validation/', transform=transform_train) if valid else None\n",
    "        tests_dataset = datasets.ImageFolder(path + '/testing/', transform=transform_test)   \n",
    "        images = train_dataset.imgs\n",
    "    except:\n",
    "        # Create a single dataset from the specified path to split later\n",
    "        dataset = datasets.ImageFolder(path, transform=transform_train)\n",
    "        generator = torch.Generator().manual_seed(42)\n",
    "        if valid:\n",
    "            train_dataset, valid_dataset, tests_dataset = random_split(dataset=dataset, lengths=[0.8, 0.1, 0.1], generator=generator)\n",
    "        else:\n",
    "            train_dataset, tests_dataset = random_split(dataset=dataset, lengths=[0.8, 0.2], generator=generator)\n",
    "        images = dataset.imgs\n",
    "    \n",
    "    # Strip the labels from the dataloader object\n",
    "    for image in images:\n",
    "        if image[1] not in dictionary['label int']:\n",
    "            dictionary['files'].append(image[0])\n",
    "            dictionary['label int'].append(image[1])\n",
    "            label_split = image[0].split('\\\\')\n",
    "            label_name = label_split[1] if len(label_split) > 2 else label_split[0].split('/')[-1]\n",
    "            dictionary['label name'].append(label_name)\n",
    "        \n",
    "    # Printing out the lengths of our datasets:\n",
    "    message = f'Training dataset length: {len(train_dataset)}\\n'\n",
    "    message += f'Validation dataset length: {len(valid_dataset)}\\n' if valid else ''\n",
    "    message += f'Testing dataset length: {len(tests_dataset)}'\n",
    "    common.Print(message)\n",
    "    \n",
    "    # Training Dataloader\n",
    "    train_dl = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    # Validation Dataloader\n",
    "    valid_dl = DataLoader(valid_dataset, batch_size, shuffle=False) if valid else None\n",
    "\n",
    "    # Testing Dataloader\n",
    "    tests_dl = DataLoader(tests_dataset, len(tests_dataset), shuffle=False)\n",
    "\n",
    "    # Print how many batches will be in our training/validation dataloaders (dependent on batch_size)\n",
    "    message = f'Number of training batches: {len(train_dl)}\\n'\n",
    "    message += f'Number of validation batches: {len(valid_dl)}\\n' if valid else ''\n",
    "    message += f'Number of testing batches: {len(tests_dl)}'\n",
    "    common.Print(message)\n",
    "    \n",
    "    labels = dictionary['label name']\n",
    "    # Display examples from each class\n",
    "    if display:\n",
    "        cols = 5\n",
    "        rows = int(len(labels) / cols) + 1\n",
    "        fig = plt.figure(figsize=(2*cols,2*rows))\n",
    "        for index, file in enumerate(dictionary['files']):\n",
    "            image = Image.open(file)\n",
    "            sub_plot = fig.add_subplot(rows, cols, index+1)\n",
    "            sub_plot.set_title(f'{index} {labels[index]}')\n",
    "            sub_plot.tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "            plt.imshow(image)\n",
    "        \n",
    "    # Display the mapping from int to string for each class\n",
    "    else:\n",
    "        message = 'Labels\\n'\n",
    "        for index, label in enumerate(labels):\n",
    "            message += f'{index} -> {label}\\n'\n",
    "        common.Print(message)\n",
    "    \n",
    "    # Return a validation dataloader\n",
    "    if valid:\n",
    "        return train_dl, valid_dl, tests_dl, labels\n",
    "    \n",
    "    # Do not return a validation dataloader\n",
    "    else:\n",
    "        return train_dl, tests_dl, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507e137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    plot_history(history)\n",
    "    A function to automate plotting all of the metrics tracked in the \n",
    "    history dictionary during training. If a validation dataset was used\n",
    "    the validation metrics are plotted on the save graph as the training\n",
    "    metrics.\n",
    "    \n",
    "    inputs:\n",
    "     - history (dictionary): The dictionary of metrics from training\n",
    "    outputs:\n",
    "     -\n",
    "\"\"\"\n",
    "def plot_history(history):\n",
    "    keys = list(history.keys())\n",
    "    epochs = len(history[keys[0]])\n",
    "    x = range(epochs)\n",
    "    val = 'val_loss' in keys\n",
    "    num_keys = int(len(keys)/2) if val else len(keys)\n",
    "    fig, ax = plt.subplots(1, num_keys, figsize=(12,4))\n",
    "    \n",
    "    for i in range(num_keys):\n",
    "        y1 = history[keys[i]]\n",
    "        ax[i].plot(x, y1, label='Train')\n",
    "        if val:\n",
    "            ax[i].plot(x, y2, label='Valid')\n",
    "        ax[i].set_xlabel('Epoch')\n",
    "        ax[i].set_ylabel(keys[i])\n",
    "        ax[i].grid(True)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943231b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    set_loss_fn(name)\n",
    "    A function to wrap the available pytorch loss functions so that\n",
    "    the training function can reference the loss function using a \n",
    "    string. For information on how each loss function works please\n",
    "    refer to pytorch's online documentation.\n",
    "    \n",
    "    inputs:\n",
    "     - name (string): The name of the loss function to use\n",
    "    outputs:\n",
    "     - loss (reference): A reference to the pytorch loss function\n",
    "\"\"\"\n",
    "def set_loss_fn(name):\n",
    "    try:\n",
    "        name = name.lower()\n",
    "        losses = {'l1':nn.L1Loss,\n",
    "                 'mse':nn.MSELoss,\n",
    "                 'crossentropy':nn.CrossEntropyLoss,\n",
    "                 'ctc':nn.CTCLoss,\n",
    "                 'nll':nn.NLLLoss,\n",
    "                 'poissonnll':nn.PoissonNLLLoss,\n",
    "                 'gaussiannll':nn.GaussianNLLLoss,\n",
    "                 'kldiv':nn.KLDivLoss,\n",
    "                 'bce':nn.BCELoss,\n",
    "                 'bcewithlogits':nn.BCEWithLogitsLoss,\n",
    "                 'marginranking':nn.MarginRankingLoss,\n",
    "                 'hingeembedding':nn.HingeEmbeddingLoss,\n",
    "                 'multilabelmargin':nn.MultiLabelMarginLoss,\n",
    "                 'huber':nn.HuberLoss,\n",
    "                 'smoothl1':nn.SmoothL1Loss,\n",
    "                 'softmargin':nn.SoftMarginLoss,\n",
    "                 'multilabelsoftmargin':nn.MultiLabelSoftMarginLoss,\n",
    "                 'cosineembedding':nn.CosineEmbeddingLoss,\n",
    "                 'multimargin':nn.MultiMarginLoss,\n",
    "                 'tripletmargin':nn.TripletMarginLoss,\n",
    "                 'tribletmarginwithdistance':nn.TripletMarginWithDistanceLoss}\n",
    "\n",
    "        loss = nn.MSELoss\n",
    "        for key in losses.keys():\n",
    "            if key == name:\n",
    "                loss = losses[key]\n",
    "                break\n",
    "\n",
    "        return loss\n",
    "\n",
    "    except Exception as e:\n",
    "        common.Print_Error('GAN -> set loss', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10abe55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    set_optimizer(name)\n",
    "    A function to wrap the available pytorch optimizer algorithms\n",
    "    so that the training function can reference the optimizer using \n",
    "    a string. For information on how each optimizer works please \n",
    "    refer to pytorch's online documentation.\n",
    "    \n",
    "    inputs:\n",
    "     - name (string): The name of the optimization algorithm to use\n",
    "    outputs:\n",
    "     - optimizer (reference): A reference to the pytorch optimizer\n",
    "\"\"\"\n",
    "def set_optimizer(name):\n",
    "    try:\n",
    "        name = name.lower()\n",
    "        optimizers = {'adadelta':optim.Adadelta,\n",
    "                      'adagrad':optim.Adagrad,\n",
    "                      'adam':optim.Adam,\n",
    "                      'adamw':optim.AdamW,\n",
    "                      'sparseadam':optim.SparseAdam,\n",
    "                      'adamax':optim.Adamax,\n",
    "                      'asgd':optim.ASGD,\n",
    "                      'lbfgs':optim.LBFGS,\n",
    "                      'nadam':optim.NAdam,\n",
    "                      'radam':optim.RAdam,\n",
    "                      'rmsprop':optim.RMSprop,\n",
    "                      'rprop':optim.Rprop,\n",
    "                      'sgd':optim.SGD}\n",
    "\n",
    "        optimizer = optim.SGD\n",
    "        for key in optimizers.keys():\n",
    "            if key == name:\n",
    "                optimizer = optimizers[key]\n",
    "                break\n",
    "                \n",
    "        return optimizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        common.Print_Error('GAN -> set optimizer', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9fdbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    test(mode, tests_dl, labels, thresh, verbose, device)\n",
    "    A function to automate the testing procedure for pytorch models. If verbose\n",
    "    is greater than 1 it is assumed that there is only one batch in the tests_dl\n",
    "    dataloader.\n",
    "    \n",
    "    inputs:\n",
    "     - model (pytorch model): The model to test\n",
    "     - tests_dl (pytorch dataloader): The data to use for testing\n",
    "     - labels (list): A list of strings specifying the text labels\n",
    "     - thresh (float): The threshold for classification, only used for binary classification\n",
    "     - verbose (int): The level of output to print\n",
    "     - device (string): The device to perform model processing on\n",
    "    outputs:\n",
    "     - accuracy (float): The models accuracy over the test dataset\n",
    "\"\"\"\n",
    "def test(model, tests_dl, labels=None, thresh=0.5, verbose=0, device='cuda:0'):\n",
    "    # Transfer model to the device\n",
    "    \n",
    "    # Start the data count and accuracy at zero\n",
    "    data_count = 0\n",
    "    accuracy = 0.0\n",
    "\n",
    "    # Set the trained model for evaluation and disable gradient updates\n",
    "    \n",
    "        for x_batch, y_batch in tests_dl: \n",
    "            # Transfer batched data to the same device as the model\n",
    "            \n",
    "            # Predict a batch of outputs using the model\n",
    "            \n",
    "            # Threshold or one-hot the prediction according to the prediction shape\n",
    "            \n",
    "            # If more information is desired output class information\n",
    "            if verbose > 0:\n",
    "                # Transfer the data to the cpu\n",
    "                y_batch = y_batch.cpu()\n",
    "                pred = pred.cpu()\n",
    "                \n",
    "                # If text labels have been provided, use them\n",
    "                if labels is not None:\n",
    "                    print(classification_report(y_batch, pred, target_names=labels))\n",
    "                    conf_mat = confusion_matrix(y_batch, pred, labels=y_batch.unique())\n",
    "                    conf_disp = ConfusionMatrixDisplay(conf_mat, labels)\n",
    "                else:\n",
    "                    print(classification_report(y_batch, pred))\n",
    "                    conf_mat = confusion_matrix(y_batch, pred, labels=y_batch.unique())\n",
    "                    conf_disp = ConfusionMatrixDisplay(conf_mat, y_batch.unique())\n",
    "                    \n",
    "                conf_disp.plot(xticks_rotation='vertical')\n",
    "    # Normalize the accuracy to [0 - 1]\n",
    "    accuracy /= data_count\n",
    "    print(f'Test Dataset Accuracy: {accuracy}')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8ef95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    train(model, num_epochs, train_dl, valid_dl=None, optimizer='adam', loss_fn='mse', lr=0.001, device='cuda:0')\n",
    "        A function to automate pytorch model training.\n",
    "        \n",
    "    inputs:\n",
    "     - model (pytorch model): The pytorch model to train\n",
    "     - num_epochs (int): The number of epochs to train the model for\n",
    "     - train_dl (dataloader): Pytorch dataloader object for training dataset\n",
    "     - valid_dl (dataloader): Pytorch dataloader object for validation dataset\n",
    "     - optimizer (string): The name of the optimizer function to use. See set_optimizer() for available functions\n",
    "     - loss_fn (string): The name of the loss function to use. See set_loss_fn() for available functions\n",
    "     - lr (float): The learning rate for the optimizer to use\n",
    "     - thresh (float): The threshold to consider an answer as class 1 (lower) or class 2 (higher)\n",
    "     - device (float): The name of the device to train on\n",
    "    outputs:\n",
    "     - history (dictionary): A dictionary of the training metrics\n",
    "     - dev_model (pytorch model): The trained model on the cpu\n",
    "\"\"\"\n",
    "def train(model, num_epochs, train_dl, valid_dl=None, optimizer='adam', loss_fn='mse', lr=0.001, thresh=0.5, device='cuda:0'):\n",
    "    \n",
    "    train_count = len(train_dl.dataset)\n",
    "    valid_count = 0 if valid_dl is None else len(valid_dl.dataset)\n",
    "    \n",
    "    # Transfer the model to the device\n",
    "\n",
    "    # A dictionary which will house metrics for both the training and validation datasets\n",
    "    history = {}\n",
    "    metrics = ['loss', 'acc'] if valid_dl is None else ['loss', 'acc', 'val_loss', 'val_acc']\n",
    "    for name in metrics:\n",
    "        history[name] = [0] * num_epochs\n",
    "    \n",
    "    # Get the function reference for the pytorch loss function\n",
    "    loss_fn = set_loss_fn(loss_fn)()\n",
    "    \n",
    "    # Get the function reference for the pytorch optimizer\n",
    "    optimizer = set_optimizer(optimizer)(dev_model.parameters(), lr=lr)\n",
    "    \n",
    "    # Initiate training for our planned number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode (compute gradients)\n",
    "\n",
    "        # Iterate through our batches (housed within our training DataLoader object)\n",
    "        for x_batch, y_batch in train_dl:\n",
    "            # Transfer batched data to the same device as the model\n",
    "\n",
    "            # Predict a batch of outputs using the model\n",
    "\n",
    "            # Compute the loss\n",
    "\n",
    "            # Backpropagate our loss\n",
    "\n",
    "            # Compute any training metrics\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float() if len(pred.shape) > 1 else ((pred >= thresh).float() == y_batch).float()\n",
    "        \n",
    "        # Compute the mean of the training metrics\n",
    "\n",
    "        # Perform validation\n",
    "        if valid_dl is not None:\n",
    "            # Set the model for evaluation and disable gradient updates\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x_batch, y_batch in valid_dl:\n",
    "                    # Transfer batched data to the same device as the model\n",
    "\n",
    "                    # Predict a batch of outputs using the model\n",
    "\n",
    "                    # Compute the loss\n",
    "\n",
    "                    # Compute any validation metrics\n",
    "                    is_correct = (torch.argmax(pred, dim=1) == y_batch).float() if len(pred.shape) > 1 else ((pred >= thresh).float() == y_batch).float()\n",
    "            \n",
    "            # Compute the mean of the validation metrics\n",
    "        \n",
    "        # Update the progress bar for training\n",
    "        common.Print_Status('Training', epoch, num_epochs, history)\n",
    "           \n",
    "    return history, dev_model.to('cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4638ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'kitchen_utensils'\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "# Use the load_data function to load the dataset\n",
    "train_dl, tests_dl, labels = load_data(path, \n",
    "                                       batch_size=16, \n",
    "                                       image_width=image_width,\n",
    "                                       image_height=image_height,\n",
    "                                       transform_train=None, \n",
    "                                       transform_test=None, \n",
    "                                       valid=False,\n",
    "                                       display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eaa442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f06acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47745207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3edced1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d332b-4671-4364-ab24-7364048d7432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
